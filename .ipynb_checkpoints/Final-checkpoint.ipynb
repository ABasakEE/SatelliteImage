{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in. pix2pix is not application specific—it can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white images, turning Google Maps photos into aerial images, and even transforming sketches into photos.\n\nIn the pix2pix cGAN, you condition on input images and generate corresponding output images.\n\nThe architecture of your network will contain:\n\n- A generator with a [U-Net](https://arxiv.org/abs/1505.04597){:.external}-based architecture.\n- A discriminator represented by a convolutional PatchGAN classifier (proposed in the [pix2pix paper](https://arxiv.org/abs/1611.07004){:.external}).\n\nNote that each epoch can take around 15 seconds on a single V100 GPU.","metadata":{}},{"cell_type":"code","source":"# import tensorflow and other libraries\nimport tensorflow as tf\nimport os\nimport pathlib\nimport time\nimport datetime\nfrom matplotlib import pyplot as plt\nfrom IPython import display","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-09T21:09:34.237864Z","iopub.status.busy":"2024-07-09T21:09:34.237493Z","iopub.status.idle":"2024-07-09T21:09:47.268346Z","shell.execute_reply":"2024-07-09T21:09:47.267385Z","shell.execute_reply.started":"2024-07-09T21:09:34.237828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the path using pathlib and load the dataset\nfrom pathlib import Path\nPATH = Path(# your code here#)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:03.797985Z","iopub.status.busy":"2024-07-09T21:10:03.797588Z","iopub.status.idle":"2024-07-09T21:10:03.802865Z","shell.execute_reply":"2024-07-09T21:10:03.801780Z","shell.execute_reply.started":"2024-07-09T21:10:03.797956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(PATH.parent.iterdir())","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:07.329424Z","iopub.status.busy":"2024-07-09T21:10:07.329038Z","iopub.status.idle":"2024-07-09T21:10:07.339985Z","shell.execute_reply":"2024-07-09T21:10:07.338944Z","shell.execute_reply.started":"2024-07-09T21:10:07.329395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find the size of the image and split it into 2 images\nsample_image = tf.io.read_file(str(PATH / 'train/1.jpg')) \nsample_image = tf.io.decode_jpeg(sample_image)\nprint(sample_image.shape)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:09.914164Z","iopub.status.busy":"2024-07-09T21:10:09.913650Z","iopub.status.idle":"2024-07-09T21:10:09.972351Z","shell.execute_reply":"2024-07-09T21:10:09.971053Z","shell.execute_reply.started":"2024-07-09T21:10:09.914125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.imshow(sample_image)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:12.125404Z","iopub.status.busy":"2024-07-09T21:10:12.125031Z","iopub.status.idle":"2024-07-09T21:10:12.629158Z","shell.execute_reply":"2024-07-09T21:10:12.627893Z","shell.execute_reply.started":"2024-07-09T21:10:12.125377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You need to separate real building facade images from the architecture label images—all of which will be of size `300 x 300`.\n\nDefine a function that loads image files and outputs two image tensors:","metadata":{}},{"cell_type":"code","source":"def load(image_file):\n  # Read and decode an image file to a uint8 tensor\n  image = tf.io.read_file(image_file)\n  image = tf.io.decode_jpeg(image)\n\n  # Split each image tensor into two tensors:\n  # - one with a real building facade image\n  # - one with an architecture label image \n  w = # your code here #\n  input_image = image[# your code here #]\n  real_image = image[# your code here #]\n\n  # Convert both images to float32 tensors\n  input_image = tf.cast(input_image, tf.float32)\n  real_image = tf.cast(real_image, tf.float32)\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:14.518349Z","iopub.status.busy":"2024-07-09T21:10:14.517598Z","iopub.status.idle":"2024-07-09T21:10:14.524906Z","shell.execute_reply":"2024-07-09T21:10:14.523835Z","shell.execute_reply.started":"2024-07-09T21:10:14.518313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot a sample of the input (architecture label image) and real (building facade photo) images:","metadata":{}},{"cell_type":"code","source":"inp, re = load(str(PATH / 'train/100.jpg'))\n# Casting to int for matplotlib to display the images\nplt.figure()\nplt.imshow(inp / 255.0)\nplt.figure()\nplt.imshow(re / 255.0)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:16.405015Z","iopub.status.busy":"2024-07-09T21:10:16.404500Z","iopub.status.idle":"2024-07-09T21:10:17.268464Z","shell.execute_reply":"2024-07-09T21:10:17.267227Z","shell.execute_reply.started":"2024-07-09T21:10:16.404977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n\nDefine several functions that:\n\n1. Resize each `300 x 300` image to a larger height and width—`330 x 330`.\n2. Randomly crop it back to `256 x 256`.\n3. Randomly flip the image horizontally i.e., left to right (random mirroring).\n4. Normalize the images to the `[-1, 1]` range.","metadata":{}},{"cell_type":"code","source":"# The maps training set consist of x images\nBUFFER_SIZE = # your code here #\n# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\nBATCH_SIZE = 1\n# final image size:\nIMG_WIDTH = # your code here #\nIMG_HEIGHT = # your code here #","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:30.598938Z","iopub.status.busy":"2024-07-09T21:10:30.598538Z","iopub.status.idle":"2024-07-09T21:10:30.604327Z","shell.execute_reply":"2024-07-09T21:10:30.603234Z","shell.execute_reply.started":"2024-07-09T21:10:30.598908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:31.806247Z","iopub.status.busy":"2024-07-09T21:10:31.805214Z","iopub.status.idle":"2024-07-09T21:10:31.813500Z","shell.execute_reply":"2024-07-09T21:10:31.812101Z","shell.execute_reply.started":"2024-07-09T21:10:31.806198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image[0], cropped_image[1]","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:34.160748Z","iopub.status.busy":"2024-07-09T21:10:34.160276Z","iopub.status.idle":"2024-07-09T21:10:34.167100Z","shell.execute_reply":"2024-07-09T21:10:34.165879Z","shell.execute_reply.started":"2024-07-09T21:10:34.160712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizing the images to [-1, 1]\ndef normalize(input_image, real_image):\n  input_image = (# your code here #) - 1\n  real_image = (# your code here #) - 1\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:36.364002Z","iopub.status.busy":"2024-07-09T21:10:36.363611Z","iopub.status.idle":"2024-07-09T21:10:36.370231Z","shell.execute_reply":"2024-07-09T21:10:36.368146Z","shell.execute_reply.started":"2024-07-09T21:10:36.363973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function()\ndef random_jitter(input_image, real_image):\n  # Resizing to 330x330\n  input_image, real_image = resize(# your code here #)\n\n  # Random cropping back to 256x256\n  input_image, real_image = random_crop(# your code here #)\n\n  if tf.random.uniform(()) > 0.5:\n    # Random mirroring\n    input_image = tf.image.flip_left_right(input_image)\n    real_image = tf.image.flip_left_right(real_image)\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:38.418862Z","iopub.status.busy":"2024-07-09T21:10:38.418464Z","iopub.status.idle":"2024-07-09T21:10:38.426009Z","shell.execute_reply":"2024-07-09T21:10:38.424639Z","shell.execute_reply.started":"2024-07-09T21:10:38.418827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Examine some of the preprocessed output:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nfor i in range(4):\n  rj_inp, rj_re = random_jitter(inp, re)\n  plt.subplot(2, 2, i + 1)\n  plt.imshow(rj_inp / 255.0)\n  plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:40.244204Z","iopub.status.busy":"2024-07-09T21:10:40.243429Z","iopub.status.idle":"2024-07-09T21:10:40.824453Z","shell.execute_reply":"2024-07-09T21:10:40.823363Z","shell.execute_reply.started":"2024-07-09T21:10:40.244167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inp.shape)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:47.192330Z","iopub.status.busy":"2024-07-09T21:10:47.191954Z","iopub.status.idle":"2024-07-09T21:10:47.199160Z","shell.execute_reply":"2024-07-09T21:10:47.196652Z","shell.execute_reply.started":"2024-07-09T21:10:47.192294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:","metadata":{}},{"cell_type":"code","source":"def load_image_train(image_file):\n  input_image, real_image = # your code here #\n  input_image, real_image = # your code here #\n  input_image, real_image = # your code here #\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:49.159256Z","iopub.status.busy":"2024-07-09T21:10:49.158858Z","iopub.status.idle":"2024-07-09T21:10:49.165005Z","shell.execute_reply":"2024-07-09T21:10:49.163870Z","shell.execute_reply.started":"2024-07-09T21:10:49.159220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_test(image_file):\n  input_image, real_image = # your code here #\n  input_image, real_image = # your code here #\n  input_image, real_image = # your code here #\n\n  return input_image, real_image","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:51.198519Z","iopub.status.busy":"2024-07-09T21:10:51.198129Z","iopub.status.idle":"2024-07-09T21:10:51.204349Z","shell.execute_reply":"2024-07-09T21:10:51.203199Z","shell.execute_reply.started":"2024-07-09T21:10:51.198488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\ntrain_dataset = train_dataset.map(load_image_train,\n                                  num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:53.032727Z","iopub.status.busy":"2024-07-09T21:10:53.032323Z","iopub.status.idle":"2024-07-09T21:10:53.389693Z","shell.execute_reply":"2024-07-09T21:10:53.388792Z","shell.execute_reply.started":"2024-07-09T21:10:53.032697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:55.509091Z","iopub.status.busy":"2024-07-09T21:10:55.508711Z","iopub.status.idle":"2024-07-09T21:10:55.724121Z","shell.execute_reply":"2024-07-09T21:10:55.723205Z","shell.execute_reply.started":"2024-07-09T21:10:55.509061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the generator\n\nThe generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597){:.external}. A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](../images/segmentation.ipynb) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/){:.external}.)\n\n- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n- There are skip connections between the encoder and decoder (as in the U-Net).","metadata":{}},{"cell_type":"markdown","source":"Define the downsampler (encoder):","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:10:58.856586Z","iopub.status.busy":"2024-07-09T21:10:58.856171Z","iopub.status.idle":"2024-07-09T21:10:58.861325Z","shell.execute_reply":"2024-07-09T21:10:58.860121Z","shell.execute_reply.started":"2024-07-09T21:10:58.856553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  # your code here #\n  return result","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:00.704248Z","iopub.status.busy":"2024-07-09T21:11:00.703893Z","iopub.status.idle":"2024-07-09T21:11:00.710879Z","shell.execute_reply":"2024-07-09T21:11:00.709716Z","shell.execute_reply.started":"2024-07-09T21:11:00.704221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(inp, 0))\nprint (down_result.shape)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:03.169671Z","iopub.status.busy":"2024-07-09T21:11:03.169276Z","iopub.status.idle":"2024-07-09T21:11:03.310181Z","shell.execute_reply":"2024-07-09T21:11:03.309152Z","shell.execute_reply.started":"2024-07-09T21:11:03.169639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the upsampler (decoder):","metadata":{}},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  # your code here #\n\n  return result","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:05.013068Z","iopub.status.busy":"2024-07-09T21:11:05.012669Z","iopub.status.idle":"2024-07-09T21:11:05.019741Z","shell.execute_reply":"2024-07-09T21:11:05.018585Z","shell.execute_reply.started":"2024-07-09T21:11:05.013037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_model = upsample(3, 4)\nup_result = up_model(down_result)\nprint (up_result.shape)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:07.247681Z","iopub.status.busy":"2024-07-09T21:11:07.246752Z","iopub.status.idle":"2024-07-09T21:11:07.313575Z","shell.execute_reply":"2024-07-09T21:11:07.312517Z","shell.execute_reply.started":"2024-07-09T21:11:07.247645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the generator with the downsampler and the upsampler:","metadata":{}},{"cell_type":"code","source":"def Generator():\n  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n\n  down_stack = [\n    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n    downsample(128, 4),  # (batch_size, 64, 64, 128)\n    downsample(256, 4),  # (batch_size, 32, 32, 256)\n    downsample(512, 4),  # (batch_size, 16, 16, 512)\n    downsample(512, 4),  # (batch_size, 8, 8, 512)\n    downsample(512, 4),  # (batch_size, 4, 4, 512)\n    downsample(512, 4),  # (batch_size, 2, 2, 512)\n    downsample(512, 4),  # (batch_size, 1, 1, 512)\n  ]\n\n  up_stack = [\n    # your code here #\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh')  # (batch_size, 256, 256, 3)\n\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    # your code here #\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    # your code here #\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:12.446654Z","iopub.status.busy":"2024-07-09T21:11:12.445782Z","iopub.status.idle":"2024-07-09T21:11:12.456586Z","shell.execute_reply":"2024-07-09T21:11:12.455421Z","shell.execute_reply.started":"2024-07-09T21:11:12.446617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the generator model architecture:","metadata":{}},{"cell_type":"code","source":"generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:20.532522Z","iopub.status.busy":"2024-07-09T21:11:20.531587Z","iopub.status.idle":"2024-07-09T21:11:22.189084Z","shell.execute_reply":"2024-07-09T21:11:22.187871Z","shell.execute_reply.started":"2024-07-09T21:11:20.532476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test the generator:","metadata":{}},{"cell_type":"code","source":"inp = tf.image.resize(inp, [256, 256],\n                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\ngen_output = generator(inp[tf.newaxis, ...], training=False)\nplt.imshow(gen_output[0, ...])","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:26.625688Z","iopub.status.busy":"2024-07-09T21:11:26.625302Z","iopub.status.idle":"2024-07-09T21:11:27.297125Z","shell.execute_reply":"2024-07-09T21:11:27.296091Z","shell.execute_reply.started":"2024-07-09T21:11:26.625659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the generator loss\n\nGANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004){:.external}.\n\n- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n- This allows the generated image to become structurally similar to the target image.\n- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper.","metadata":{}},{"cell_type":"code","source":"LAMBDA = 100","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:30.349585Z","iopub.status.busy":"2024-07-09T21:11:30.348875Z","iopub.status.idle":"2024-07-09T21:11:30.353995Z","shell.execute_reply":"2024-07-09T21:11:30.352890Z","shell.execute_reply.started":"2024-07-09T21:11:30.349549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:31.981634Z","iopub.status.busy":"2024-07-09T21:11:31.981229Z","iopub.status.idle":"2024-07-09T21:11:31.986533Z","shell.execute_reply":"2024-07-09T21:11:31.985412Z","shell.execute_reply.started":"2024-07-09T21:11:31.981604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total loss = GAN loss + (lambda * mean absolute error)","metadata":{}},{"cell_type":"code","source":"def generator_loss(disc_generated_output, gen_output, target):\n  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n  # Mean absolute error\n  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n  total_gen_loss = # your code here #\n\n  return total_gen_loss, gan_loss, l1_loss","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:33.856869Z","iopub.status.busy":"2024-07-09T21:11:33.856137Z","iopub.status.idle":"2024-07-09T21:11:33.861894Z","shell.execute_reply":"2024-07-09T21:11:33.860894Z","shell.execute_reply.started":"2024-07-09T21:11:33.856833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training procedure for the generator is as follows:","metadata":{}},{"cell_type":"markdown","source":"![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n","metadata":{}},{"cell_type":"markdown","source":"## Build the discriminator\n\nThe discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004){:.external}.\n\n- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n- The discriminator receives 2 inputs: \n    - The input image and the target image, which it should classify as real.\n    - The input image and the generated image (the output of the generator), which it should classify as fake.\n    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n\n  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n  # Downsampling #\n  down1 = # your code here #\n  down2 = # your code here #\n  down3 = # your code here #\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n\n  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n\n  return tf.keras.Model(inputs=[inp, tar], outputs=last)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:37.518288Z","iopub.status.busy":"2024-07-09T21:11:37.517924Z","iopub.status.idle":"2024-07-09T21:11:37.527761Z","shell.execute_reply":"2024-07-09T21:11:37.526418Z","shell.execute_reply.started":"2024-07-09T21:11:37.518255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:40.986037Z","iopub.status.busy":"2024-07-09T21:11:40.985074Z","iopub.status.idle":"2024-07-09T21:11:41.222263Z","shell.execute_reply":"2024-07-09T21:11:41.221307Z","shell.execute_reply.started":"2024-07-09T21:11:40.985999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\nplt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar()","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:44.244660Z","iopub.status.busy":"2024-07-09T21:11:44.243715Z","iopub.status.idle":"2024-07-09T21:11:44.678300Z","shell.execute_reply":"2024-07-09T21:11:44.677311Z","shell.execute_reply.started":"2024-07-09T21:11:44.244623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the discriminator loss\n\n- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n- The `total_loss` is the sum of `real_loss` and `generated_loss`.","metadata":{}},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:47.304584Z","iopub.status.busy":"2024-07-09T21:11:47.303688Z","iopub.status.idle":"2024-07-09T21:11:47.309459Z","shell.execute_reply":"2024-07-09T21:11:47.308382Z","shell.execute_reply.started":"2024-07-09T21:11:47.304546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training procedure for the discriminator is shown below.\n\nTo learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004){:.external}.","metadata":{}},{"cell_type":"markdown","source":"![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n","metadata":{}},{"cell_type":"markdown","source":"## Define the optimizers and a checkpoint-saver\n","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:49.790627Z","iopub.status.busy":"2024-07-09T21:11:49.789648Z","iopub.status.idle":"2024-07-09T21:11:49.803189Z","shell.execute_reply":"2024-07-09T21:11:49.802042Z","shell.execute_reply.started":"2024-07-09T21:11:49.790588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:11:52.619334Z","iopub.status.busy":"2024-07-09T21:11:52.618372Z","iopub.status.idle":"2024-07-09T21:11:52.625186Z","shell.execute_reply":"2024-07-09T21:11:52.624054Z","shell.execute_reply.started":"2024-07-09T21:11:52.619296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate images\n\nWrite a function to plot some images during training.\n\n- Pass images from the test set to the generator.\n- The generator will then translate the input image into the output.\n- The last step is to plot the predictions and _voila_!","metadata":{}},{"cell_type":"markdown","source":"Note: The `training=True` is intentional here since you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want).","metadata":{}},{"cell_type":"code","source":"def generate_images(model, test_input, tar):\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15, 15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # Getting the pixel values in the [0, 1] range to plot.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:12:54.300714Z","iopub.status.busy":"2024-07-09T21:12:54.300319Z","iopub.status.idle":"2024-07-09T21:12:54.307761Z","shell.execute_reply":"2024-07-09T21:12:54.306505Z","shell.execute_reply.started":"2024-07-09T21:12:54.300681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_target, example_input in test_dataset.take(1):\n  generate_images(generator, example_input, example_target)\n  generate_images(generator, example_target, example_input)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:12:57.035889Z","iopub.status.busy":"2024-07-09T21:12:57.035495Z","iopub.status.idle":"2024-07-09T21:12:58.807959Z","shell.execute_reply":"2024-07-09T21:12:58.807004Z","shell.execute_reply.started":"2024-07-09T21:12:57.035859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\n- For each example input generates an output.\n- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n- Next, calculate the generator and the discriminator loss.\n- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n- Finally, log the losses to TensorBoard.","metadata":{}},{"cell_type":"code","source":"log_dir=\"logs/\"\n\nsummary_writer = tf.summary.create_file_writer(\n  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:13:02.117964Z","iopub.status.busy":"2024-07-09T21:13:02.117457Z","iopub.status.idle":"2024-07-09T21:13:02.127697Z","shell.execute_reply":"2024-07-09T21:13:02.126610Z","shell.execute_reply.started":"2024-07-09T21:13:02.117931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(input_image, target, step):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    gen_output = generator(input_image, training=True)\n\n    disc_real_output = discriminator([input_image, target], training=True)\n    disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n  generator_gradients = gen_tape.gradient(gen_total_loss,\n                                          generator.trainable_variables)\n  discriminator_gradients = disc_tape.gradient(disc_loss,\n                                               discriminator.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))\n\n  with summary_writer.as_default():\n    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:13:05.593897Z","iopub.status.busy":"2024-07-09T21:13:05.593487Z","iopub.status.idle":"2024-07-09T21:13:05.604749Z","shell.execute_reply":"2024-07-09T21:13:05.603562Z","shell.execute_reply.started":"2024-07-09T21:13:05.593867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n\n- Iterates over the number of steps.\n- Every 10 steps print a dot (`.`).\n- Every 1k steps: clear the display and run `generate_images` to show the progress.\n- Every 5k steps: save a checkpoint.","metadata":{}},{"cell_type":"code","source":"def fit(train_ds, test_ds, steps):\n  example_target, example_input = next(iter(test_ds.take(1)))\n  start = time.time()\n\n  for step, (target, input_image) in train_ds.repeat().take(steps).enumerate():\n    # your code here #\n\n    # Training step\n    if (step+1) % 10 == 0:\n      print('.', end='', flush=True)\n\n\n    # Save (checkpoint) the model every 5k steps\n    if (step + 1) % 5000 == 0:\n      checkpoint.save(file_prefix=checkpoint_prefix)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:13:11.228643Z","iopub.status.busy":"2024-07-09T21:13:11.227938Z","iopub.status.idle":"2024-07-09T21:13:11.236065Z","shell.execute_reply":"2024-07-09T21:13:11.235066Z","shell.execute_reply.started":"2024-07-09T21:13:11.228610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir {log_dir}","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:13:16.444010Z","iopub.status.busy":"2024-07-09T21:13:16.443598Z","iopub.status.idle":"2024-07-09T21:13:22.980229Z","shell.execute_reply":"2024-07-09T21:13:22.979202Z","shell.execute_reply.started":"2024-07-09T21:13:16.443979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(train_dataset, test_dataset, steps=40000)","metadata":{"execution":{"iopub.execute_input":"2024-07-09T21:13:22.982147Z","iopub.status.busy":"2024-07-09T21:13:22.981799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n\n- Check that neither the generator nor the discriminator model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n- The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n- For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.\n- For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.\n- As training progresses, the `gen_l1_loss` should go down.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}